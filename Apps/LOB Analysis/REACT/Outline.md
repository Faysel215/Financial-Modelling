### LOBSTER Data Analysis Application: System Outline

### 1. Project Goal & Architecture

**Objective:** To create a robust, end-to-end system that ingests raw, high-frequency limit order book (LOB) data, processes it to derive meaningful quantitative metrics, stores it efficiently, and presents it in an interactive web-based dashboard for analysis.

**System Architecture (Three-Tier Model):**
1. Data Processing Layer (Simulated Hadoop/Spark): A Python environment responsible for large-scale data transformation and enrichment.
2. Backend & Storage Layer (API & Database): A Python web server that manages the database and serves data through a REST API, with TimescaleDB acting as the specialized time-series data store.
3. Frontend/Presentation Layer (Web UI): A browser-based React application that provides interactive visualizations and dashboards for the end-user.

This architecture effectively separates concerns, allowing each layer to be developed, scaled, and maintained independently.
### 2. The Data Pipeline: From Raw Files to Database

This is a two-stage process orchestrated by two distinct Python scripts.
#### Stage 1: Data Processing & Feature Engineering
- Script: process_data.py
- Input: Two raw LOBSTER CSV files (e.g., SPY_..._message_50.csv and SPY_..._orderbook_50.csv).
- Core Responsibilities:
	1. File Discovery: Automatically finds the _message_ and _orderbook_ files in its directory.
	2. Chunk-Based Processing: Reads the massive source files in manageable chunks (e.g., 1,000,000 rows at a time) to prevent memory errors, simulating how a distributed system like Hadoop/Spark would operate.
	3. Data Merging: Combines the message and order book data row-by-row.
	4. Cleaning & Transformation:
		- Converts the LOBSTER time (seconds since midnight) into a full, timezone-aware datetime object.
		- Scales integer-based price columns to their proper decimal format (e.g., 911400 -> 91.14).
	5. Metric Calculation (Feature Engineering): For each event, it computes essential quantitative metrics:
		- Mid-Price: The average of the best bid and ask.
		- Bid-Ask Spread: The difference between the best ask and bid.
		- Market Depth: Total volume available at the top 5 levels.
		- Order Book Imbalance (OBI): A measure of buy vs. sell pressure.
	6. Sampling for Development: Includes a DATA_PERCENTAGE_TO_PROCESS flag to allow developers to quickly process a small subset of the data for testing.
		- Output:
6. A single, clean CSV file (..._processed_metrics.csv) containing the enriched data.
7. A pointer file (last_processed_file.txt) that holds the name of the output CSV, creating a reliable link to the next stage.
#### Stage 2: Database Setup and Ingestion
- **Script:** `api_server.py`
- **Input:** The _processed_metrics.csv file generated by Stage 1 (found via the pointer file).
- **Core Responsibilities (runs on startup):**
	1. Database Creation: Connects to the PostgreSQL server and creates the quant_data database if it doesn't already exist.
	2. Table Initialization: Connects to the quant_data database and creates the lob_metrics table with the correct schema. It also enables the timescaledb extension and converts the table into a hypertable, which is critical for time-series query performance.
	3. Automated Data Ingestion: Checks if the lob_metrics table is empty. If so, it reads the pointer file to find the correct processed CSV and uses PostgreSQL's highly efficient COPY command to bulk-load the data.
### 3. Backend Implementation (API)
- **Script:** `api_server.py`
- **Framework:** Flask (a lightweight Python web framework).
- Core Responsibilities (after initialization):
	1. Run a Web Server: Starts a local web server (typically on port 5001).
	2. Expose REST Endpoints: Provides several URL endpoints that the React frontend can call to request data.
		- /api/metrics/1s: Fetches the key quantitative metrics (Mid-Price, Spread, Depth, OBI), aggregated into 1-second buckets for efficient visualization.
		- /api/volume/executed: Calculates and returns the total executed volume for buy-side and sell-side trades.
	3. Database Interaction: Each endpoint function connects to the TimescaleDB, executes a specific SQL query, fetches the results (using pandas for convenience), and formats them as JSON to send back to the frontend.
### 4. Frontend Implementation (UI)

- Primary File: App.js
- Framework: React (a JavaScript library for building user interfaces).
- Core Responsibilities:
	1. Component-Based Structure: The UI is broken down into reusable components (e.g., TimeSeriesChart, VolumeChart).
	2. Data Fetching: When the application loads, it uses the browser's fetch API to make HTTP requests to the backend endpoints (e.g., http://localhost:5001/api/metrics/1s).
	3. State Management: Uses React's useState and useEffect hooks to manage the application's state (e.g., storing the fetched data, tracking loading status).
	4. Interactive Visualization: Uses the Recharts library to render the JSON data into interactive charts and graphs, allowing the user to hover over data points to see details.
	5. Dashboard Layout: Presents all the charts and tables in a clean, grid-based dashboard layout for easy analysis.
### 5. End-to-End Workflow
1. User places raw data: The user drops the two LOBSTER CSV files into the project folder.
2. User runs Stage 1: The user executes python process_data.py. The script processes the raw files and creates the clean ..._processed_metrics.csv and last_processed_file.txt.
3. User runs Stage 2: The user executes python api_server.py. The server starts, creates the database and table, ingests the processed data, and begins listening for API requests.
4. User runs UI: The user executes npm start in the React app's directory.
5. Browser opens: The React application loads in the browser.
6. UI requests data: The React components automatically make fetch calls to the running API server.
7. API serves data: The Flask server queries the TimescaleDB and returns the requested data as JSON.
8. UI displays data: The React components receive the JSON and render the interactive charts, completing the flow from raw file to visual analysis.